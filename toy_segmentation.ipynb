{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nptyping import NDArray\n",
    "import random\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Set, Dict, Tuple, Optional, Any, Callable\n",
    "from PIL import Image\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_image_to_arr, display_rgb, SVD2D\n",
    "import dim_reduction\n",
    "from spectral_data import ImageSpectralData\n",
    "dim_reduction.init_basis_cache()\n",
    "import cityscapes_helper\n",
    "from dataset_creation import (\n",
    "    MultiScaleImageSampler, \n",
    "    ImageChunkSummarizerOptions, \n",
    "    MultiScaleImageEncoder, \n",
    "    MultiScaleImageDecoder, \n",
    "    ImageAtScale, \n",
    "    CityscapesDatasetFactory, \n",
    "    WholeImage,\n",
    "    CityScapesDataset\n",
    ")\n",
    "from cityscapes_helper import format_semantic_composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ben Desptop\n",
    "#cityscapes_helper.set_visual_data_zip_path(\"C:\\\\Users\\\\Ben\\\\Downloads\\\\leftImg8bit_trainvaltest.zip\")\n",
    "#cityscapes_helper.set_semantic_data_zip_path(\"C:\\\\Users\\\\Ben\\\\Downloads\\\\gtFine_trainvaltest.zip\")\n",
    "\n",
    "#Ben Laptop (via WSL)\n",
    "cityscapes_helper.set_visual_data_zip_path(\"/mnt/c/Users/benki/Downloads/leftImg8bit_trainvaltest.zip\")\n",
    "cityscapes_helper.set_semantic_data_zip_path(\"/mnt/c/Users/benki/Downloads/gtFine_trainvaltest.zip\")\n",
    "\n",
    "cityscapes_helper.initialize_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.npy\n",
      "1.npy\n",
      "2.npy\n",
      "3.npy\n",
      "4.npy\n",
      "5.npy\n",
      "6.npy\n",
      "(30000, 30)\n",
      "(30000, 701)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benthehuman/school/vision/UW-Vision-Segmentation/dataset_creation.py:581: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_01 = (features - feature_min) / feature_range\n"
     ]
    }
   ],
   "source": [
    "dataset = CityScapesDataset(\"full_dataset_v1_30000\")\n",
    "dataset.load()\n",
    "normalized_features = dataset.get_normalized_features()\n",
    "print(dataset.labels.shape)\n",
    "print(dataset.features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_feature_subset = [(3, 'center_loc_polar'), (3, 'mean')]\n",
    "toy_feature_mask = dataset.get_feature_subset_mask(toy_feature_subset)\n",
    "tiny_features = dataset.features[:, toy_feature_mask]\n",
    "tiny_features_normalized = dataset.get_normalized_features()[:, toy_feature_mask]\n",
    "tiny_features_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4699\n"
     ]
    }
   ],
   "source": [
    "foliage_index = 21\n",
    "foliage_composition = dataset.labels[:, foliage_index]\n",
    "majority_foliage_mask = foliage_composition > 0.5\n",
    "print(majority_foliage_mask.sum()) # 4699 of 30000 features are mostly foliage.. sounds a bit high but whatever.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(tiny_features_normalized, majority_foliage_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at S.executeCodeCell (/home/benthehuman/.vscode-server-insiders/extensions/ms-toolsai.jupyter-2021.10.1001414422/out/client/extension.js:66:301742)",
      "at S.execute (/home/benthehuman/.vscode-server-insiders/extensions/ms-toolsai.jupyter-2021.10.1001414422/out/client/extension.js:66:300732)",
      "at S.start (/home/benthehuman/.vscode-server-insiders/extensions/ms-toolsai.jupyter-2021.10.1001414422/out/client/extension.js:66:296408)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/benthehuman/.vscode-server-insiders/extensions/ms-toolsai.jupyter-2021.10.1001414422/out/client/extension.js:66:312326)",
      "at async t.CellExecutionQueue.start (/home/benthehuman/.vscode-server-insiders/extensions/ms-toolsai.jupyter-2021.10.1001414422/out/client/extension.js:66:311862)"
     ]
    }
   ],
   "source": [
    "gpc.score(tiny_features_normalized, majority_foliage_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def anchor_grid(largest_scale: int, smallest_scale: int) -> List[List[Tuple[int, int]]]:\n",
    "    CITYSCAPES_Y = 1024\n",
    "    CITYSCAPES_X = 2048\n",
    "\n",
    "    #total_margin = largest_scale - smallest_scale\n",
    "    num_rows = (CITYSCAPES_Y - largest_scale) // smallest_scale\n",
    "    num_cols = (CITYSCAPES_X - largest_scale) // smallest_scale\n",
    "    result = []\n",
    "    for row_i in range(num_rows):\n",
    "        curr_row = []\n",
    "        anchor_y = row_i * smallest_scale\n",
    "        for col_i in range(num_cols):\n",
    "            anchor_x = col_i * smallest_scale\n",
    "            curr_row.append([anchor_y, anchor_x])\n",
    "        result.append(curr_row)\n",
    "    return result\n",
    "\n",
    "def multiscale_image_grid(anchor_grid: List[List[Tuple[int, int]]], sampler: MultiScaleImageSampler, img: NDArray[Any]) -> List[List[List[ImageAtScale]]]:\n",
    "    #flat_anchors =  [item for sublist in anchor_grid for item in sublist]\n",
    "    #flat_multiscale_images = [sampler.sample(img, anchor) for anchor in flat_anchors]\n",
    "    i = 0\n",
    "    result = []\n",
    "    for anchor_row in anchor_grid:\n",
    "        image_row = []\n",
    "        for anchor in anchor_row:\n",
    "            multiscaleImages = sampler.sample(img, anchor)\n",
    "            multiscaleImages.insert(0, WholeImage(img))\n",
    "            image_row.append(multiscaleImages)\n",
    "        result.append(image_row)\n",
    "    return result\n",
    "\n",
    "def feature_grid(image: NDArray[Any], encoder: MultiScaleImageEncoder, scales: List[int]):\n",
    "    sampler = MultiScaleImageSampler(scales)\n",
    "    ag = anchor_grid(scales[0], scales[-1])\n",
    "    image_grid = multiscale_image_grid(ag, sampler, image)\n",
    "    nrows = len(image_grid)\n",
    "    ncols = len(image_grid[0])\n",
    "    print(nrows, ncols)\n",
    "    print(image_grid[0][0])\n",
    "\n",
    "    dumbo = [[scaleImage] for scaleImage in image_grid[0][0]]\n",
    "    first_feature = encoder.encode(dumbo)\n",
    "    feature_size = first_feature.shape[1]\n",
    "\n",
    "    wumb = 0\n",
    "    result = np.zeros((len(image_grid), len(image_grid[0]), feature_size))\n",
    "    for row_i in range(nrows):\n",
    "        for col_i in range(ncols):\n",
    "            print(wumb, end=\", \")\n",
    "            wumb += 1\n",
    "            multiscale_image = image_grid[row_i][col_i]\n",
    "            dumbo = [[scaleImage] for scaleImage in multiscale_image]\n",
    "            result[row_i, col_i, :] =  encoder.encode(dumbo)[0]\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "s = [240, 96, 32]\n",
    "\n",
    "whole_img_options = ImageChunkSummarizerOptions()\n",
    "whole_img_options.downsample_factor = 8\n",
    "whole_img_options.n_pcm_coeffs = [80, 40, 30]\n",
    "\n",
    "opt_240 = ImageChunkSummarizerOptions()\n",
    "opt_240.downsample_factor = 3\n",
    "opt_240.n_pcm_coeffs = [120, 40, 20]\n",
    "\n",
    "opt_96 = ImageChunkSummarizerOptions()\n",
    "opt_96.downsample_factor = 2\n",
    "opt_96.n_pcm_coeffs = [120, 40, 20]\n",
    "\n",
    "opt_32 = ImageChunkSummarizerOptions()\n",
    "opt_32.downsample_factor = 1\n",
    "opt_32.n_pcm_coeffs = [60, 20, 15]\n",
    "\n",
    "chunk_summarizer_options = [whole_img_options, opt_240, opt_96, opt_32]\n",
    "\n",
    "multiscaleEncoder = MultiScaleImageEncoder(chunk_summarizer_options)\n",
    "print(multiscaleEncoder.n_scales)\n",
    "aceneid, img = cityscapes_helper.loadRandomVisualInfo()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 56\n",
      "[<dataset_creation.WholeImage object at 0x7fa91c82f280>, <dataset_creation.ImageAtScale object at 0x7fa91c82fb50>, <dataset_creation.ImageAtScale object at 0x7fa91c82fe20>, <dataset_creation.ImageAtScale object at 0x7fa91c82f8b0>]\n",
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a4b240b4472d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiscaleEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-925d149b465d>\u001b[0m in \u001b[0;36mfeature_grid\u001b[0;34m(image, encoder, scales)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mmultiscale_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mdumbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscaleImage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscaleImage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmultiscale_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdumbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/school/vision/UW-Vision-Segmentation/dataset_creation.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, scale_chunks)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0mfeature_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_scale_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/school/vision/UW-Vision-Segmentation/dataset_creation.py\u001b[0m in \u001b[0;36mencode_summaries\u001b[0;34m(self, chunks)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_total_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msummarizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarizers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0mfeature_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/school/vision/UW-Vision-Segmentation/dataset_creation.py\u001b[0m in \u001b[0;36mcalculate_summary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mspec_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_spectral_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pcm_pca_coeffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpc_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_coeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/school/vision/UW-Vision-Segmentation/spectral_data.py\u001b[0m in \u001b[0;36mget_pcm_pca_coeffs\u001b[0;34m(self, pcm_index, num_coeffs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_pcm_pca_coeffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcm_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_coeffs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdim_reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_basis_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_square\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbasis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_basis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim2D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/school/vision/UW-Vision-Segmentation/dim_reduction.py\u001b[0m in \u001b[0;36minit_basis_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupported_basis_dims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pcm_bases/dim_{bd}.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mbasis_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbd\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPartialOrthoBasis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0minit_whole_image_basis_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/packages/anaconda3/envs/phosvd_env/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    441\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/packages/anaconda3/envs/phosvd_env/lib/python3.9/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0m_check_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfortran_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_array_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/packages/anaconda3/envs/phosvd_env/lib/python3.9/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Cannot parse header: {!r}\\nException: {!r}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/packages/anaconda3/envs/phosvd_env/lib/python3.9/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36msafe_eval\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m    976\u001b[0m     \"\"\"\n\u001b[1;32m    977\u001b[0m     \u001b[0;31m# Local import to speed up numpy's import time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "feature_grid(img, multiscaleEncoder, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
